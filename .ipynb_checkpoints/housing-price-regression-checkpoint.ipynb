{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Housing Prices with Regression  ------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: The Ames Housing Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ames.jpg\" width=\"60%\">\n",
    "\n",
    "The Ames Housing Data Set contains information from the Ames Assessor’s Office used in computing the value of individual residential properties sold in Ames, Iowa from 2006 to 2010, as well as the actual eventual sale prices for the properties.  \n",
    "\n",
    "The data set contains information for more than 2900 properties.  The **[data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)** outlines more than 75 descriptive variables. Some are **nominal** (categorical), meaning they are non-numerical and lack clear-cut order (Examples: Neighborhood, Type of roofing).  Some are **ordinal**, meaning they are categorical but have a clear order (Example:  Heating Quality (Excellent, Good, Average, Poor)). Some are **discrete**, meaning they are numerical but at set intervals (Year Built, Number of Fireplaces). The rest are **continuous**, meaning they are numerical and can theoretically take any value in a range (1st Floor Square Feet).\n",
    "\n",
    "In this project, I have attempted to craft as accurate a model as possible for predicting housing sale prices, using regression techniques, enhanced by feature engineering, feature selection, and regularization.  This project was connected to a private Kaggle competition.  The Ames data set was divided into a labeled training set, and an unlabeled test set (the labels exist, but are withheld).  I aim to accurately predict the withheld sale prices for the test set, based on the features and prices in the training set.  This exercise mimics real-world modeling scenarios.  \n",
    "\n",
    "As in many Kaggle competitions, I upload a set of predictions and receive a score that evaluates my performance on a subset of the test set, but evaluation on the entire test set does not take place until the end of the competition.  This is significant, because it is possible to overfit your model to the subset of the test set that is being used to generate scores. **It is important to prioritize generalizability in a model**, instead of just over-tweaking to maximize your accuracy score on the available subset of test data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of data cleaning and data wrangling were involved in preparing the data for analysis and modeling.  The full code for this part of the project, and the subsequent editing of the test data to match, can be viewed **[here in NBViewer](https://nbviewer.jupyter.org/gist/eamonious/d82a505fcd346af7a9e69d861c4337ad)**.  \n",
    "\n",
    "Among the changes:\n",
    "\n",
    "- **Nominal variables were one-hot-encoded**, meaning they were converted into a series of binary variables representing whether each category was true for a given house, yes (1) or no (0). \n",
    "- **Ordinal variables were mapped to numerical format.**  At times this can be subjective. If one category is significantly better than the others for example, a simple {0,1,2,3} mapping may not be as appropriate as say, {0,1,2,5}.  In other cases, it may be most accurate to use something like {-2,-1,0,1,2}.  Looking at the counts for each category can be helpful.  In cases where the great majority of houses are in one category, it is often sensible to map this category to 0, and indicate other categories as above or below this \"standard\".  In general, using the data dictionary and then individual research as needed is essential here.\n",
    "- **Null or missing values were addressed**, typically either by removing properties with missing data or imputing values for the missing data.  Some imputations were trivial: Houses with no garage had null values for all garage-related variables, these were imputed easily (Add a 'None' category for 'Garage Type', add 0 for Garage Area, etc.).  Other imputations were more complex:  Many values were missing for Lot Frontage (i.e.; the length in feet over which a property is adjacent to the street). 0 was not a sensible imputation here, the houses must have *some* lot frontage. Imputing the mean lot frontage from all houses was an option, but seemed very rough.  I chose instead to impute the mean lot frontage from the neighborhood in which the house was located.\n",
    "\n",
    "At the end of this process I had a feature table with over 200 variables, and this is prior to any attempts at feature engineering.  I have included images of the variables most highly correlated with sale price, both positively and negatively.  Keep in mind that these may not all be present among the most predictive variables, as some may be highly collinear and cancel out each other's influence.  However we would certainly expect *some* of these top variables to be among our most predictive variables in any final model.  \n",
    "\n",
    "<img src=\"images/correlation-top-variables.png\">\n",
    "\n",
    "The full heatmap for correlation with sale price can be viewed **[here](../images/projects/housing/correlation-heatmap.png)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Sale Price with Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **supervised machine learning**, we build a model to predict a target variable from a set of feature variables, by training it on individuals where the target variable is already known.  Supervised machine learning problems can be broadly divided into two types of problems:  **classification** problems and **regression** problems.  Classification problems involve trying to predict a categorical target variable - we want to classify new individuals into one of n categories.  Regression problems involve trying to predict a continuous target variable - we want to predict the value of the variable for each new individual.  As sale price is continuous, this is a regression problem. \n",
    "\n",
    "Regression analysis essentially involves identifying a mapping function that can be applied to a set of feature variable values to generate an estimate of the true target value for a given individual (i.e.; a prediction).  This mapping function is derived by minimizing some **loss function** that indicates the performance accuracy of the mapping function.  The smaller the total loss in our predictions, the closer our predictions are to being correct on average, and the better the performance of the model overall.  \n",
    "\n",
    "The most basic regression technique available to us is **linear regression**.  In this case our mapping function is limited to the form of a straight line:\n",
    "\n",
    "$$ \\begin{eqnarray} y &=& \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\\\ \\end{eqnarray} $$ \n",
    "\n",
    "In this equation, y represents our target prediction for each individual, the different values of X are the values of the feature variables for each individual, and the different values of $ \\beta $ are coefficients that represent the magnitude (positive or negative) of each feature variable's influence on our prediction.  $ \\beta_0 $ represents a constant, the y-intercept of our line; it is also theoretically our prediction for the value of a house where all feature variables = 0.  \n",
    "\n",
    "I will use **sklearn's LinearRegression() method** to run my linear regression.  When I run this, sklearn basically identifies the **line of best fit** for the data it receives; that is, the line (in multidimensional space) which minimizes loss between the line and the target values - the line which gives the best predictions overall for the training data.  The standard loss function used in linear regression is **mean squared error**, or the sum of the squares of the error between the predictions (the line) and the actual target values.  This is why we often hear this referred to as least squares regression; the line of best fit is identified by finding the line that minimizes mean squared error.\n",
    "\n",
    "The image below depicts a line of best fit for a single feature variable, using the least squares technique.\n",
    "<img src=\"\">\n",
    "\n",
    "Each time we run LinearRegression(), it will return the best available line of fit from what it receives, plain and simple. This is the first part of minimizing loss.  The second part, and really the part that we have control over, involves **iteratively improving our model by changing the features that we include** when we run LinearRegression().  This can be accomplished by engineering new features to add predictive value, or eliminating redundant and irrelevant features to prevent overfitting.  This may seem simple at first blush, but with hundreds of features and the ability to engineer many more, it can be quite complicated.  I will go into this below.\n",
    "\n",
    "After making such changes and establishing a new line of best fit, we can evaluate the performance of our model by checking the **R2 Score**, which essentially calculates the squared error around our fit line as a percentage of the inherent variance in the data and subtracts that from 1.  The R2 score ranges from 0 to 1, and can be thought of as the percentage of variation in the data that is explained by the predictive power of the model.  The higher the score, the better the performance.\n",
    "\n",
    "**The ultimate goal here is to iteratively minimize the loss in our model's predictions for new data.**  To do this, we want to minimize the loss we observe in our training data, while also preserving generalizability.  These principles extend to even the most complex supervised machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and read in cleaned training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Import model validation and preprocessing packages\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV , RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "#Import feature selection packages\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_regression\n",
    "\n",
    "#Import regression and metric packages\n",
    "from sklearn.metrics import r2_score, recall_score, make_scorer, f1_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#Jupyter magics (for notebook visualizations)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "#Set viewing max to 300 variables\n",
    "pd.set_option('max_columns',300)\n",
    "\n",
    "#Reads in the cleaned training data\n",
    "X = pd.read_csv('./datasets/data_clean_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step should be regarded as optional, but was important to the overall outcome of the Kaggle competition.  **The data dictionary mentions five observations (houses) that are significant outliers**.  Essentially these were in-family sales where the sale price greatly underestimates the actual value of the home.  The mean squared error for these observations is therefore very large, enough that it significant skews the line of best fit.  \n",
    "\n",
    "The training set contains three of these five values.  The test set contains two.  However, both are in the withheld portion of the test set.  As a result, if you were trying to win the Kaggle competition, it was necessary to leave these outliers in when building your model, so as to account for their influence on the line of best fit.  However, leaving them in greatly disrupts loss minimization on the available test subset (that contains no outliers) - in other words, leaving the outliers out allowed you to score much better in the Kaggle rankings.  \n",
    "\n",
    "In my opinion, leaving the outliers in was a flaw in the competition design, because, while instructive about the limitations of loss functions in the presence of outliers, the undersold houses are anomalous cases that take away from the real-world task of trying to accurately assess the price of homes.  I worked around the issue by submitting one model with the outliers and one model without (two submissions were permitted).  For the purposes of demonstrating iterative improvements in model accuracy, I will proceed here with the outliers *excluded*.  You want to do this now, before manipulating the data any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes the three extreme Price by Area outliers in the training set\n",
    "X.drop(959,inplace=True)\n",
    "X.drop(1882,inplace=True)\n",
    "X.drop(125,inplace=True)\n",
    "\n",
    "#Save csv with no outliers\n",
    "X.to_csv('./datasets/data_clean_no_outliers.csv',index=False)\n",
    "#Load the new csv in\n",
    "X = pd.read_csv('./datasets/data_clean_no_outliers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the Target and Feature Variables\n",
    "\n",
    "By convention, the target variable (here the Sale Price) is represented by a lowercase y (because it is a single vector), and the feature variables are represented with an uppercase X (because they comprise a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets target variable (Sale Price) as y\n",
    "y = X['SalePrice']\n",
    "\n",
    "#Sets feature variables (everything but Sale Price and Id) as X\n",
    "X.drop(['SalePrice','Id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering simply involves creating new variables from those that you already have.  These variables can often add predictive value.  I did some basic feature engineering during the initial data processing stage; for example, using the month-of-sale variable to create a season-of-sale variable, which I felt could be more relevant.  \n",
    "\n",
    "A more systematic approach to feature engineering is the **PolynomialFeatures** method in sklearn's preprocessing suite. This generates all possible polynomial combinations of a set of features below a specified nth degree (default = 2). In this case we will generate new features from the **square of each feature** (some variables may vary quadratically with sale price instead of linearly, in which case these new features may be more predictive than their first-degree counterparts).  We will also generate new features from the **product of every two features**.  This type of engineered feature is called an **interaction term**, because it represents the interaction effect between two variables.  If two variables have a synergistic effect on sale price, for instance, then the product of the two features may add considerable predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2045, 22577)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generates the full polynomial feature table.  \n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overall Qual Gr Liv Area       0.876299\n",
       "Bsmt Qual Gr Liv Area          0.843971\n",
       "Overall Qual 1st Flr SF        0.842331\n",
       "Overall Qual^2                 0.833020\n",
       "Overall Qual Total Bsmt SF     0.830507\n",
       "Overall Qual Garage Area       0.827364\n",
       "Overall Qual Bsmt Qual         0.824825\n",
       "Overall Qual Garage Cars       0.824217\n",
       "Total Bsmt SF Gr Liv Area      0.822287\n",
       "Overall Qual Year Built        0.812162\n",
       "Gr Liv Area Garage Area        0.811653\n",
       "Overall Qual TotRms AbvGrd     0.811520\n",
       "Overall Qual Garage Yr Blt     0.811197\n",
       "Overall Qual Year Remod/Add    0.809928\n",
       "Gr Liv Area Garage Cars        0.809253\n",
       "Gr Liv Area Exter Qual 2       0.806486\n",
       "Overall Qual                   0.805477\n",
       "Overall Qual Yr Sold           0.805454\n",
       "Gr Liv Area Kitchen Qual 2     0.800986\n",
       "Garage Area Kitchen Qual 2     0.799663\n",
       "dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adds appropriate feature names to all polynomial features\n",
    "X_poly = pd.DataFrame(X_poly,columns=poly.get_feature_names(X.columns))\n",
    "\n",
    "#Generates list of poly feature correlations\n",
    "X_poly_corrs = X_poly.corrwith(y)\n",
    "#Shows features most highly correlated (positively) with sale price, poly features included\n",
    "X_poly_corrs.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yr Sold Garage Type_Detchd         -0.369763\n",
       "Garage Type_Detchd                 -0.369751\n",
       "Garage Type_Detchd^2               -0.369751\n",
       "Year Built Garage Type_Detchd      -0.369113\n",
       "Garage Yr Blt Garage Type_Detchd   -0.368788\n",
       "dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows features most highly correlated (negatively) with sale price, poly features included\n",
    "X_poly_corrs.sort_values().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PolynomialFeatures on our feature set (>200 features to start with) results in **over 20,000 features**!  We can see that **some of them are showing higher correlation with sale price than *any* of our original variables**.  However, these top poly feature variables will often be highly collinear, and most poly features will be completely irrelevant.  The vast majority of features will need to be removed, or our model will end up incredibly overfit to the data.  This is where feature elimination comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Elimination and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small feature set, feature selection can be done manually.  One might remove individual features and observe how the training score is affected.  Removing a feature will always reduce the training score some amount, but if that amount is very small, the feature can be discarded in favor of model generalizability.  One could continue eliminating the least influential variable until only variables that significantly affect training score remain.  However, with a feature set like the one we have here, this approach isn't feasible.\n",
    "\n",
    "I experimented with different subsets of polynomial features extensively, and ultimately chose to use just two of the highest correlated polynomial features.  This seemed to return the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the polynomial features I ended up including.  \n",
    "#I decided to just create them manually here, since they were so few.\n",
    "\n",
    "#Interaction between Overall Quality and Above Ground Living Area (sq. ft)\n",
    "X['Overall Qual Gr Liv Area'] = X['Overall Qual'] * X['Gr Liv Area']\n",
    "#Square of Overall Quality\n",
    "X['Overall Qual^2'] = X['Overall Qual'] * X['Overall Qual']\n",
    "\n",
    "#I also engineered a feature for total bathrooms from full baths and half baths.\n",
    "X['Bathroom total'] = X['Full Bath'] + (0.5 * X['Half Bath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discuss L1, L2 Regularization... as well as ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes the 70 poorest correlated features (by absolute magnitude):\n",
    "features_keep = list(abs(X.corrwith(y)).sort_values(ascending=False)[:-100].index)\n",
    "\n",
    "X = X[features_keep]\n",
    "\n",
    "X.drop(['Exterior 1st_CemntBd', 'Foundation_CBlock', 'Neighborhood_dummies_Timber', 'Alley', 'Electrical',\n",
    "        'Lot Config_Inside', 'Neighborhood_dummies_BrDale', 'Exterior 2nd_HdBoard', 'Exterior 1st_MetalSd',\n",
    "        'Neighborhood_dummies_NAmes', 'Neighborhood_dummies_BrkSide', 'House Style_2Story', 'MS SubClass_50','Roof Style_Hip',\n",
    "        'Neighborhood_dummies_Sawyer', 'Mas Vnr Type_BrkFace','MS SubClass_60','Fence','Foundation_Slab','Exterior 2nd_CmentBd',\n",
    "        'Bedroom AbvGr','Lot Shape','Exterior 2nd_MetalSd','Garage Type_BuiltIn','Garage Finish'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split and Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical workflow for building a model based on supervised learning from a training set involves creating what is called a **train/test split**. Essentially we are splitting the training data into a training bloc and a testing bloc. We will train the model on the properties in the train bloc and then test it on the properties in the test bloc.  This test bloc is not to be confused with the test set for the Kaggle competition - think of that test set as equivalent to actual real world data that one might want to predict.  This test bloc is a subdivision of our training set that we will use to validate our training process.\n",
    "\n",
    "The point of creating this train/test split is to make sure that our model generalizes well to new data.  If we simply train on the entire training data set, then our model will likely become **overfit** to the particular data in that set, such that it may actually be worse at predicting the sale price for new houses.  Once we are satisfied that the model we've built generalizes well to new data, *then* we can train it on the full training set (including our test bloc) to incorporate as much information as possible into the model before running it on the test set and submitting the predictions to Kaggle.\n",
    "\n",
    "When we run train_test_split(X,y), it will create these blocs by randomly splitting the observations in the dataset into one or the other.  \n",
    "Discuss train test split randomization process and default split size...\n",
    "\n",
    "Discuss StandardScaler normalizer in context of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Just explain them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Explain here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show side by side image of improvements..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_1 = {\n",
    "    'alpha':[.1,.3,.5,.6,.7,.8,1,1.5],\n",
    "    'l1_ratio':[0,.3,.5,.7,1]\n",
    "}\n",
    "\n",
    "gs_simple = GridSearchCV(ElasticNet(),param_grid_1,cv=5,verbose=1)\n",
    "\n",
    "gs_simple.fit(X_train_sc,y_train_log)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_simple.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the R2 score of the best parameters\n",
    "gs_simple.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of coefficients, sorted\n",
    "pd.DataFrame(gs_simple.best_estimator_.coef_,index=X_train.columns).sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a list of the model's predictions\n",
    "preds = gs_simple.predict(X_test_sc)\n",
    "\n",
    "#Plots predicted values against actual values in the train/test split.\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(np.exp(preds),y_test,marker = '+')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Actual values')\n",
    "plt.plot([0,np.max(y_test)],[0,np.max(y_test)], c = 'k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors (KNN) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn method in the neighbors subpackage\n",
    "\n",
    "Analogous to the more famous K Nearest Neighbors Classifier method\n",
    "\n",
    "The regression prediction is based in calculating an average position of the k nearest neighbors in multidimensional space.\n",
    "So imagine multidimensional coordinates based on the combination of a house’s X variable values.  The training set provides a bunch of examples of coordinate combinations matched with target prices.  For the test set, where the target is unknown, each test house has known X-coordinates and unknown price.  The KNN model takes the k nearest X-coordinate neighbors from the training set and returns the average of their prices as the predicted price for the test house.\n",
    "\n",
    "This involves certain variables:  weight (uniform v distance); method of distance calculation, and of course number of neighbors k.\n",
    "\n",
    "Implementation with GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Code\n",
    "parameters = [{'weights': ['uniform', 'distance'],'n_neighbors': range(2,50,5),'p':range(1,3)}]\n",
    "\n",
    "knnreg = GridSearchCV(KNeighborsRegressor(), parameters, cv=5, scoring='r2',n_jobs=3, verbose=2)\n",
    "\n",
    "knnreg.fit(X_train_sc, y_train_log) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = knnreg.predict(X_test_sc)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(np.exp(preds),y_test,marker = '+')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Actual values')\n",
    "plt.plot([0,np.max(y_test)],[0,np.max(y_test)], c = 'k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The Importance of Outliers\n",
    "\n",
    "As I mentioned earlier on, the five outlier homes had a significant effect on final model performance.  I submitted one model with the outliers *excluded*, which allowed me to score accurately on the ranking board (and have a sense of how well my iterative changes were performing on the pure data), and one model with the outliers *included*, because I suspected this would be necessary to perform well in the final evaluation on the whole test set.  \n",
    "\n",
    "85 participants submitted models to the competition.  My model *without outliers* ranked **2nd** in accuracy on the subset of the test data used for scoring, which contained no outliers.  My model without outliers would have ranked in the low 20s at this point (only top scores were ranked).\n",
    "\n",
    "In the actual competition results, evaluated on the entire test set, which included two outliers, my model *with outliers* was my top performing model, and ranked **5th** overall.  Clearly, the influence of the outliers on mean squared error was predominant, but the underlying model appears to have been sound.  I was the only participant ranked in the top 10 on both boards.\n",
    "\n",
    "\n",
    "Discuss the explainability advantage of linear regression versus more complex techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
